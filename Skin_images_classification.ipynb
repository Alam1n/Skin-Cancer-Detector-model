{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 6 files from Benign to the test set.\n",
      "Moved 48 files from Malignant to the test set.\n",
      "Data split complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "base_dir = \"archive (9)\"\n",
    "train_dir = os.path.join(base_dir, \"train_cancer\")\n",
    "test_dir = os.path.join(base_dir, \"testing_cancer\")\n",
    "\n",
    "# Subfolders for classes\n",
    "classes = [\"Benign\", \"Malignant\"]\n",
    "\n",
    "# Create testing_cancer directory and class subdirectories\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "for class_name in classes:\n",
    "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "\n",
    "# Split percentage (e.g., 20% of data goes to the test set)\n",
    "split_ratio = 0.2\n",
    "\n",
    "# Loop through each class folder\n",
    "for class_name in classes:\n",
    "    # Get the list of files in the class directory\n",
    "    class_train_dir = os.path.join(train_dir, class_name)\n",
    "    files = os.listdir(class_train_dir)\n",
    "    \n",
    "    # Shuffle the files\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    # Calculate the split index\n",
    "    split_index = int(len(files) * split_ratio)\n",
    "    \n",
    "    # Move the files to the test directory\n",
    "    for file_name in files[:split_index]:\n",
    "        src = os.path.join(class_train_dir, file_name)\n",
    "        dst = os.path.join(test_dir, class_name, file_name)\n",
    "        shutil.move(src, dst)\n",
    "    \n",
    "    print(f\"Moved {split_index} files from {class_name} to the test set.\")\n",
    "\n",
    "print(\"Data split complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benign: 24 files\n",
      "malignant: 192 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths\n",
    "train_dir = \"archive (9)/train_cancer\"\n",
    "\n",
    "# Subfolders for classes\n",
    "classes = [\"benign\", \"malignant\"]\n",
    "\n",
    "# Dictionary to hold the file counts\n",
    "file_counts = {}\n",
    "\n",
    "# Loop through each class folder and count the number of files\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(train_dir, class_name)\n",
    "    files = os.listdir(class_dir)\n",
    "    \n",
    "    # Count the number of files\n",
    "    file_counts[class_name] = len(files)\n",
    "\n",
    "# Print the results\n",
    "for class_name, count in file_counts.items():\n",
    "    print(f\"{class_name}: {count} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define directory and image shape\n",
    "Train_dir = \"archive (9)/train_cancer\"\n",
    "Test_dir = \"archive (9)/testing_cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 216 files belonging to 2 classes.\n",
      "Found 54 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(Train_dir,\n",
    "                                                                 image_size= (154,154),\n",
    "                                                                 batch_size =32)\n",
    "\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(Test_dir,\n",
    "                                                                 image_size= (154,154),\n",
    "                                                                 batch_size =32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 216 files belonging to 2 classes.\n",
      "Found 54 files belonging to 2 classes.\n",
      "Class Weights: {0: 4.5, 1: 0.5625}\n",
      "Epoch 1/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - accuracy: 0.4321 - loss: 0.8468 - val_accuracy: 0.8889 - val_loss: 0.4022\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 516ms/step - accuracy: 0.7860 - loss: 0.4262 - val_accuracy: 0.4444 - val_loss: 0.7793\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 827ms/step - accuracy: 0.7494 - loss: 0.3463 - val_accuracy: 0.8704 - val_loss: 0.3418\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 784ms/step - accuracy: 0.9033 - loss: 0.2739 - val_accuracy: 0.8889 - val_loss: 0.2658\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 811ms/step - accuracy: 0.8922 - loss: 0.2325 - val_accuracy: 0.8148 - val_loss: 0.4023\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 893ms/step - accuracy: 0.8611 - loss: 0.2003 - val_accuracy: 0.9259 - val_loss: 0.2338\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 770ms/step - accuracy: 0.9215 - loss: 0.1713 - val_accuracy: 0.8889 - val_loss: 0.2877\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 744ms/step - accuracy: 0.9321 - loss: 0.1272 - val_accuracy: 0.9074 - val_loss: 0.2388\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 836ms/step - accuracy: 0.9344 - loss: 0.1806 - val_accuracy: 0.9259 - val_loss: 0.1911\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 777ms/step - accuracy: 0.9529 - loss: 0.1138 - val_accuracy: 0.9444 - val_loss: 0.1552\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 508ms/step - accuracy: 0.9317 - loss: 0.1729\n",
      "Test Loss, Test Accuracy: [0.15520140528678894, 0.9444444179534912]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Define paths and image shape\n",
    "Train_dir = \"archive (9)/train_cancer\"\n",
    "Test_dir = \"archive (9)/testing_cancer\"\n",
    "IMG_shape = (154, 154)\n",
    "\n",
    "# Load datasets\n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(Train_dir,\n",
    "                                                                 image_size=IMG_shape,\n",
    "                                                                 batch_size=32)\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(Test_dir,\n",
    "                                                                 image_size=IMG_shape,\n",
    "                                                                 batch_size=32)\n",
    "\n",
    "# Function to get labels from dataset\n",
    "def get_labels(dataset):\n",
    "    labels = []\n",
    "    for images, label_batch in dataset:\n",
    "        labels.extend(label_batch.numpy())\n",
    "    return np.array(labels)\n",
    "\n",
    "# Get labels for computing class weights\n",
    "train_labels = get_labels(train_data)\n",
    "class_names = train_data.class_names  # ['Benign', 'Malignant']\n",
    "classes = np.arange(len(class_names))  # Array of class indices\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=classes,\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# Convert class weights to dictionary\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    "# Build and compile the model\n",
    "base_model = EfficientNetB0(input_shape=(154, 154, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(class_names), activation='softmax')  # Adjusted for multi-class classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(train_data,\n",
    "                    validation_data=test_data,\n",
    "                    epochs=10,\n",
    "                    class_weight=class_weights_dict)  # Apply class weights\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(test_data)\n",
    "print(\"Test Loss, Test Accuracy:\", results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
